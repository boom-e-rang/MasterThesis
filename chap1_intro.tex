

Shared control algorithms have been developed for robotic assistance and robot-supported training in applications ranging from assisted vehicle navigation~\cite{autovehicle2014} and surgical robotics~\cite{surgeryrobot2004,RSS_surgery2014} to brain-computer interface manipulation~\cite{teleBCI2017} and exoskeleton-assisted gait~\cite{RSS_exo2013, exoMPC2011}. The aims and safety requirements of these systems vary greatly, but one challenge is often the same---how do we most effectively allocate control between the user and the machine? 

User trust in the robot is a critical factor in the overall performance of the joint human-machine system \cite{measurement_of_trust}. Trust, in this context, mainly depends on robot performance and its attributes, such as dependability, predictability, and level of automation \cite{trust_factors}. Thus, to encourage user trust, shared control algorithms should avoid robot behavior that is difficult for the human to understand~\cite{RSS_RoboObj2017}, unpredictable, or unnecessary. In task-based assistance, avoiding such behavior can be challenging, because there are often many ways of accomplishing a task and the individual is likely to take an approach that is different from the controllerâ€™s calculated strategy. However, the issue can be addressed by developing shared control paradigms that instantenously adjust to operator performance and real-time adapt to varying user strategies, such as in~\cite{RSS_shah_adaptive} and in the work proposed here.

Another factor to consider is user preference. In~\cite{RSS_userpref2017}, machine learning techniques were used to model user preferences for autonomous systems; in other approaches, users were able to manually choose their level of assistance~\cite{exoskeletons}. In general, studies show that users of assistive devices prefer to maintain as much control authority as possible~\cite{argall_keynote,RSS2011_assistedteleoperation,lankenau2001}. Overconstraining user inputs may prevent them from utilizing valid control strategies and hence leads to frustration and device abandonment~\cite{survey_deviceabandonment}. That said, users may be willing to accept some loss of control authority, but only if the improvements in performance are significant~\cite{RSS2011_assistedteleoperation, lankenau2001}. Therefore, devices are most likely to be used if they make tasks significantly easier without limiting users' actions~\cite{survey_deviceabandonment}. 

% For instance, strict obstacle avoidance controls prevent wheelchair users from making maneuvers that bring them too close to a wall~\cite{lankenau2001}.

Finally, in robotic training, providing too much assistance or overconstraining users undermines the therapeutic impact of the device. Engaging the user has been shown to be critical to robotic training~\cite{marchal2009}. Likewise, allowing mistakes or errors and overall task failure has been shown to be critical to learning~\cite{thoroughman2000learning}. In some cases, successful interfaces have adopted strategies of amplifying error~\cite{emken2007motor, patton2006evaluation}. On the flip side, interfaces that provide too much support oftentimes lead to user passivity, resulting in low skill retention and little transferable skills~\cite{schmidt1992new, winstein1994effects}. 

Many shared control schemes adapt their level of support online~\cite{emken2008,riener2005,wolbrecht2008, ellis2009} using an algorithm or schedule that prescribes changes based on some notion of the user's need for assistance. These levels of support can be modulated based on performance measures such as error~\cite{fisher2014,marchal2009, reinkensmeyer2016intro,patton2006error}, movement speed~\cite{kahn2004}, and task adeptness~\cite{krebs2003}, or based on the user's strength and fitness level~\cite{lokomat_clinical_study,RSS_exo2013} or current cognitive engagement in the task \cite{assistance_in_distraction}. At other times, the level of support can be manually adjusted by a physical therapist or the users themselves \cite{exoskeletons}. These adaptive shared control schemes are often referred to as assist-as-needed and are characterized by dynamically updating the relative contributions of the robot and human.

Here, we present a filter-based assist-as-needed shared control paradigm. While other assist-as-needed schemes adjust the level of provided support from trial-to-trial, we implement a filter that acts based on real-time assessment of user intent. It requires no predefined trajectory, runs on an indefinite time horizon, and automatically adapts to operator skill. \textit{A key contribution of this work is the criterion used for evaluating and selecting admissible user input.} As our criterion, we use the Mode Insertion Gradient (MIG)---a concept from hybrid control that allows us to assess how users' inputs affect the human-machine system over a time window into the future. 

The result is a mechanical filter that remains transparent when users are skilled and high performing, allowing inputs that are safe and/or do not hinder achieving a task goal, but interferes when users are underperforming, disallowing inputs that are unsafe or incorrect with respect to the task objective. It is worth noting that the filter requires user input and does not drive the system when the user is not actively engaged in the task, encouraging user engagement. It also does not require pre-defining a desired trajectory, allowing subjects to explore various strategies for completing a task. 

In chapters to follow, we present experimental results on the performance and training effects of this real-time assist-as-needed shared control scheme. We evaluate the MIG-based filter in two modes: assistance and training. In the assistance mode, we replace ``incorrect" user inputs with a controller-calculated action to impose safety and/or task completion. In the training mode, we reject ``incorrect" user actions to allow failure and facilitate learning. Experimental and theoretical analyses of the shared control scheme in both modes are presented in Chapters~\ref{chapter: assistance}~and~\ref{chapter: training}, respectively. We demonstrate the assist-as-needed feature of the filter by evaluating correlations between initial user skill level and current user performance on the one side and controller engagement on the other. We conclude our work and suggest future steps with a discussion in Chapter~\ref{chapter: conclusions}. A large part of the results was published in June 2018 in the Proceedings of Robotics: Science and Systems \cite{myRSS_MIGMDA}. 

%Through a healthy human subject study, we show a correlation between user skill-level and the acceptance rate of the algorithm. Because we do not simply compare the user and controller decision at each time instance, we avoid the pitfall of arbitrarily rejecting actions that do not align with the controller's strategy but otherwise bring the system closer to a target configuration. In a sense, trust in the user is implicitly represented in the algorithm through the instantaneous assessment of user actions. Therefore, there is no need to implement an adaptive scheme that explicitly assesses user trustworthiness over time. Finally, in the human subject study ($n=28$), we observe that a MIG-based filter exhibits a training effect compared to training with no assistance for the tested group; in simulation, we demonstrate that a filtering algorithm utilizing a MIG criterion succeeds in task completion even with Gaussian noise inputs for two dynamic tasks---cart-pendulum inversion and SLIP balancing, while intervening minimally and remaining flexible with respect to the user's approach strategy. 
